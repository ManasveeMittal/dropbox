{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 40.464s.\n"
     ]
    }
   ],
   "source": [
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.684s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.611s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [int(_) for _ in \"15  12  8   8   7   7   7   6   5   3\".split()]\n",
    "y = [int (_) for _ in \"10  25  17  11  13  17  20  13  9   15\".split()]\n",
    "z = zip(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 10),\n",
       " (12, 25),\n",
       " (8, 17),\n",
       " (8, 11),\n",
       " (7, 13),\n",
       " (7, 17),\n",
       " (7, 20),\n",
       " (6, 13),\n",
       " (5, 9),\n",
       " (3, 15))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.145\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "print(round(pearsonr(x,y)[0],3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pearsonr in module scipy.stats.stats:\n",
      "\n",
      "pearsonr(x, y)\n",
      "    Calculates a Pearson correlation coefficient and the p-value for testing\n",
      "    non-correlation.\n",
      "    \n",
      "    The Pearson correlation coefficient measures the linear relationship\n",
      "    between two datasets. Strictly speaking, Pearson's correlation requires\n",
      "    that each dataset be normally distributed, and not necessarily zero-mean.\n",
      "    Like other correlation coefficients, this one varies between -1 and +1\n",
      "    with 0 implying no correlation. Correlations of -1 or +1 imply an exact\n",
      "    linear relationship. Positive correlations imply that as x increases, so\n",
      "    does y. Negative correlations imply that as x increases, y decreases.\n",
      "    \n",
      "    The p-value roughly indicates the probability of an uncorrelated system\n",
      "    producing datasets that have a Pearson correlation at least as extreme\n",
      "    as the one computed from these datasets. The p-values are not entirely\n",
      "    reliable but are probably reasonable for datasets larger than 500 or so.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : (N,) array_like\n",
      "        Input\n",
      "    y : (N,) array_like\n",
      "        Input\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    r : float\n",
      "        Pearson's correlation coefficient\n",
      "    p-value : float\n",
      "        2-tailed p-value\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    http://www.statsoft.com/textbook/glosp.html#Pearson%20Correlation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pearsonr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def print_tree(tree, filename):\n",
    "    '''\n",
    "        A method to save the parsed NLTK tree to a PS file\n",
    "    '''\n",
    "    # create the canvas\n",
    "    canvasFrame = nltk.draw.util.CanvasFrame()\n",
    "\n",
    "    # create tree widget\n",
    "    widget = nltk.draw.TreeWidget(canvasFrame.canvas(), tree)\n",
    "\n",
    "    # add the widget to canvas\n",
    "    canvasFrame.add_widget(widget, 10, 10)\n",
    "\n",
    "    # save the file\n",
    "    canvasFrame.print_to_file(filename)\n",
    "\n",
    "    # release the object\n",
    "    canvasFrame.destroy()\n",
    "\n",
    "# two sentences from the article\n",
    "sentences = ['Washington state voters last fall passed Initiative 594', 'The White House also said it planned to ask Congress for $500 million to improve mental health care, and Obama issued a memorandum directing federal agencies to conduct or sponsor research into smart gun technology that reduces the risk of accidental gun discharges.']\n",
    "\n",
    "# the simplest possible word tokenizer\n",
    "sentences = [s.split() for s in sentences]\n",
    "\n",
    "# part-of-speech tagging\n",
    "sentences = [nltk.pos_tag(s) for s in sentences]\n",
    "\n",
    "# pattern for recognizing structures of the sentence\n",
    "pattern = '''\n",
    "  NP: {<DT|JJ|NN.*|CD>+}   # Chunk sequences of DT, JJ, NN\n",
    "  VP: {<VB.*><NP|PP>+}     # Chunk verbs and their arguments\n",
    "  PP: {<IN><NP>}           # Chunk prepositions followed by NP\n",
    "'''\n",
    "\n",
    "# identify the chunks\n",
    "NPChunker = nltk.RegexpParser(pattern)\n",
    "chunks = [NPChunker.parse(s) for s in sentences]\n",
    "\n",
    "# save to file\n",
    "print_tree(chunks[0], '../../Data/Chapter09/charts/sent1.ps')\n",
    "print_tree(chunks[1], '../../Data/Chapter09/charts/sent2.ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Data/Chapter09/ST_gunLaws.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-54aa5f4e16af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mguns_laws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../Data/Chapter09/ST_gunLaws.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguns_laws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Data/Chapter09/ST_gunLaws.txt'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# read the text\n",
    "guns_laws = '../../Data/Chapter09/ST_gunLaws.txt'\n",
    "\n",
    "with open(guns_laws, 'r') as f:\n",
    "    article = f.read()\n",
    "\n",
    "# load NLTK modules\n",
    "sentencer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "stemmer = nltk.PorterStemmer()\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# split the text into sentences\n",
    "sentences = sentencer.tokenize(article)\n",
    "\n",
    "words = []\n",
    "stemmed_words = []\n",
    "lemmatized_words = []\n",
    "\n",
    "# and for each sentence\n",
    "for sentence in sentences:\n",
    "    # split the sentence into words\n",
    "    words.append(tokenizer.tokenize(sentence))\n",
    "\n",
    "    # stemm the words\n",
    "    stemmed_words.append([stemmer.stem(word) \n",
    "        for word in words[-1]])\n",
    "\n",
    "    # and lemmatize them\n",
    "    lemmatized_words.append([lemmatizer.lemmatize(word) \n",
    "        for word in words[-1]])\n",
    "\n",
    "# and save the results to files\n",
    "file_words  = '../../Data/Chapter09/ST_gunLaws_words.txt'\n",
    "file_stems  = '../../Data/Chapter09/ST_gunLaws_stems.txt'\n",
    "file_lemmas = '../../Data/Chapter09/ST_gunLaws_lemmas.txt'\n",
    "\n",
    "with open(file_words, 'w') as f:\n",
    "    for w in words:\n",
    "        for word in w:\n",
    "            f.write(word + '\\n')\n",
    "\n",
    "with open(file_stems, 'w') as f:\n",
    "    for w in stemmed_words:\n",
    "        for word in w:\n",
    "            f.write(word + '\\n')\n",
    "\n",
    "with open(file_lemmas, 'w') as f:\n",
    "    for w in lemmatized_words:\n",
    "        for word in w:\n",
    "            f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
